\documentclass[10pt, english]{article}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{changepage}
\usepackage{setspace}
\usepackage[margin=0.875in]{geometry}
\usepackage{multicol}
\usepackage{xcolor}

\begin{document}
\title{Reflections on Chapter 10}
\date{}
\author{}

\maketitle


\section*{General Thoughts}
Another chapter I've looked forward to. The use of clever encoding techniques to perform better regressions and analyze non numeric data is obviously very useful. I've used and coded autoencoders and touched on ResNet before, but this is actually the first reading I've done on 
NLP, so that's a very useful contribution from this chapter. The excercises in this chapter were rather easy, but I was struggling a bit with the dependencies for the second notebook. This part of machine learning, in terms of using encoders in an instrumental fashion, NLP and image
processing lies at an interesting point in the field where some math heavy papers and thinking about what priors are implied by choices in the activation functions and network design, while also featuring a huge amount of hand, or purely result based tuning, in choosing fine tuning approaches, network design,
number of layers, choosing a loss function and so on. 

All in all a good chapter, but I would have liked to see a bit deeper theory, and even more information on the state of the art and recent developments, though that would of course expand the size of the chapter, and also risk the chapter becoming dated since the field moves fast.


\section*{Space for future reading}
This chapter inspired me to read papers on NLP, specifically to go through the Word2Vec, BERT, and ELMo papers, as well as reread the famous "Attention is all you need". I also want to read up on variational autoencoders, which I've heard a lot about, and used some default implementations of,
as well as skimmed information on, but not read the formalism and math behind it, finally I also want to read through "SkipNet: Learning Dynamic Routing ...." and maybe something from the physics informed side as well when I'm on that track (due to the links between euler integraion, pde solving and residual networks)
\end{document}