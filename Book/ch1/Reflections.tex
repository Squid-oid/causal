\documentclass[10pt, english]{article}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{amsfonts} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{titlesec}
\usepackage{changepage}
\usepackage{setspace}
\usepackage[margin=0.875in]{geometry}
\usepackage{multicol}
\usepackage{xcolor}

\begin{document}
\title{Reflections on Chapter 1}
\date{}

\maketitle


\section*{General Thoughts}
This chapter consisted of largely recapping statistical tools I've already worked with heavily. 
It is always good to recap things, and be exposed to varying notations though.

I haven't seen Kitagawa-Oaxaca-Blinder decomps before, or really partialling out in general, while I can see how it can be useful to
some degree and for some cases I don't nessecarily see what is gained by stating it so explicitly when all the properties are already implied
by using a linear model.
\section*{Space for future reading}
In most cases I've worked through there have been either clear indicators of what reasonable transformations exist, or room 
for trial and error and digging through the data distributions to come to conclusions about which transformations are useful. Then there
are also tools for eliminating constructed and raw regressors from use (A/BIC, Variance inflation, F tests, ANOVA etc.) but in terms of automatically
generating justified flexible approximations I've seen little. It would be interesting to look at for instance Tysbakov which is sourced for approximation theory 
and non parametric statistical learning.

\end{document}