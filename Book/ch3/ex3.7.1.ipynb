{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1d60fc",
   "metadata": {},
   "source": [
    "### Ex 3.7.1\n",
    "The goal of this exercise is to analyitically derive the Lasso problem solution with one regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d56ed2",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd2a0f",
   "metadata": {},
   "source": [
    "We wish to minimize the function L such that,\n",
    "$$L = |\\beta|\\lambda + \\sum_n (Y_i - \\beta X_i)^2$$\n",
    "where, $\\beta$ is a fitted coefficient, $\\lambda$ a given constant termed the penalty, $Y$ a vector of values (observations), and $X$ a vector of explanatory variables. Note that $M_i$ is and will be used to indicate the i-th element of vector $M$.\n",
    "\n",
    "To begin with we can expand the exponentiation\n",
    "$$|\\beta|\\lambda + \\sum_n (Y_i^2 - Y_i \\beta X_i + \\beta^2X_i^2)$$\n",
    "then we can say that solving this is equivilant to solving the problem without the constants $Y_i^2$ added,\n",
    "$$|\\beta|\\lambda + \\sum_n  \\beta^2X_i^2 - Y_i \\beta X_i$$\n",
    "identify the sum as matrix multiplications\n",
    "$$|\\beta|\\lambda + \\beta^2X^TX - 2Y^TX\\beta$$\n",
    "\n",
    "Now we must consider three cases:\n",
    "1. $\\beta > 0$\n",
    "2. $\\beta < 0$\n",
    "3. $\\beta = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bc3aa",
   "metadata": {},
   "source": [
    "#### Case 1.\n",
    "Minimize in $\\beta$\n",
    "$$\\beta\\lambda + \\beta^2X^TX - 2Y^TX\\beta$$\n",
    "We can see that this as a simple quadratic. \n",
    "$$X^TX\\beta^2 + 2(\\lambda - Y^TX)\\beta$$\n",
    "Take the derivative to find inflection points\n",
    "$$\\frac{d L}{d \\beta} = X^TX \\beta + \\lambda - Y^TX$$\n",
    "So the inflection point is at\n",
    "$$\\beta = \\frac{-\\lambda + Y^TX}{X^TX}$$\n",
    "\n",
    "And this solution is valid if $\\lambda < Y^TX$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fe67b",
   "metadata": {},
   "source": [
    "#### Case 2.\n",
    "Minimize in $\\beta$\n",
    "$$-\\beta\\lambda + \\beta^2X^TX - 2Y^TX\\beta$$\n",
    "We can see that this as a simple quadratic. \n",
    "$$X^TX\\beta^2 - (\\lambda + Y^TX)\\beta$$\n",
    "Take the derivative to find inflection points\n",
    "$$\\frac{d L}{d \\beta} = X^TX \\beta - \\lambda - Y^TX$$\n",
    "So the inflection point is at\n",
    "$$\\beta = \\frac{\\lambda + Y^TX}{X^TX}$$\n",
    "\n",
    "And this solution is valid if $Y^TX < -\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d4bab",
   "metadata": {},
   "source": [
    "#### Case 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1aba7",
   "metadata": {},
   "source": [
    "If $\\beta = 0$ then $L = 0$.\n",
    "\n",
    "This Case can always be true, and if we consider the region covered by Case 1 and the region covered by Case 2 we can see that if $Y^TX > -\\lambda$ and $Y^TX < \\lambda$, we are forced to default to this case.\n",
    "\n",
    "But since this loss is zero, when the other two options are available they will be prefferable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d553c",
   "metadata": {},
   "source": [
    "#### Bringing the Cases together\n",
    "We can identify the OLS solution $\\frac{Y^TX}{X^TX}$ in the above cases, name this $\\beta_{ols}$.\n",
    "\n",
    "Then assuming from here orthonormalization, $X^TX$ = 1.\n",
    "\n",
    "$$\\beta = \\begin{cases}\n",
    "-\\lambda + \\beta_{ols} & \\text{if } & \\beta_{ols} > \\lambda  \\\\\n",
    "\\lambda + \\beta_{ols}  & \\text{if } & \\beta_{ols} < -\\lambda \\\\\n",
    "0 & \\text{if } & -\\lambda \\leq \\beta_{ols} \\leq \\lambda\\\\\n",
    "\\end{cases}$$ \n",
    "\n",
    "Recalling that $\\lambda$ is pos. def. \n",
    "$$\\beta = \\begin{cases}\n",
    "-\\text{sign}(\\beta_{ols})\\lambda + \\beta_{ols}& \\text{if } & \\beta_{ols} \\notin [-\\lambda, \\lambda]\\\\\n",
    "0 & \\text{if } & \\beta_{ols} \\in [-\\lambda, \\lambda]\\\\ \n",
    "\\end{cases}$$\n",
    "\n",
    "With some more rewriting \n",
    "\n",
    "$$\\beta = \\begin{cases}\n",
    "\\text{sign}(\\beta_{ols})(|\\beta_{ols}| - \\lambda) & \\text{if } & \\beta_{ols} \\notin [-\\lambda, \\lambda]\\\\\n",
    "0 & \\text{if } & \\beta_{ols} \\in [-\\lambda, \\lambda]\\\\ \n",
    "\\end{cases}$$\n",
    "\n",
    "And finally using that if $\\beta_{ols} \\in [-\\lambda, \\lambda]$ then $(|\\beta_{ols}| - \\lambda) \\leq 0$.\n",
    "\n",
    "$$\\beta = \\text{sign}(\\beta_{ols})(|\\beta_{ols}| - \\lambda)^+$$\n",
    "\n",
    "where $(..)^+ = \\text{max}(..,0)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d3baa",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "So then the Lasso operator, when working on one regressor, which has been normalized, will reduce the intensity of the fit by $\\lambda$ towards zero, and fit nothing if the OLS fit is less than $\\lambda$. If a problem (in the exception) uses an explanatory matrix X where X is orthonormal, then this will hold for each regressor individually, this is trivially shown with the Frisch-Waugh-Lovell decomp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
